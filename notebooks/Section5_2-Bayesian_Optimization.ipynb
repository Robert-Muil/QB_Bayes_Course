{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization.\n",
    "\n",
    "A critical task in most machine learning or probabilistic programming pipelines is the optimization of model hyperparameters. Several strategies can be used for function optimization, such as randomly sampling the parameter space (**random search**) or systematically evaluating the parameter space (**grid search**). This is often not trivial, because the loss function for a particular parameter can be noisy and non-linear, and for most problems we are omptimizing a set of parameters simultaneously, which can result in a high-dimensional, non-convex problem that is difficult to evaluate. Moreover, for large problems and complex models (e.g. deep neural networks) a single model run can be expensive and time-consuming. As a result, doing systematic searches over the hyperparameter space is infeasible, and random searches are usually ineffective.\n",
    "\n",
    "To circumvent this, Bayesian optimization offers a principled and efficient approach for directing a search of arbitrary global optimization problems. It involves constructing a probabilistic model of the objective function, and then using an auxiliary function, called an **acquisition function**, to obtain candidate values for evaluation using the true objective function.\n",
    "\n",
    "Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given model on a validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Global function optimization involves finding the minimum (maximum) of a function of interest. **Samples** are drawn from the domain and evaluated by the **objective function** to give a score or **cost**. These samples are candidate optimal values, which are compared to previous samples based on their cost. While the objective function may be simple to specify mathematically and in code, it can be computationally challenging to compute, and its form may be non-linear and multi-dimensional. Moreover, its solution may be non-convex, implying that a discovered mimimum value may not be a global minimum. \n",
    "\n",
    "Specific to data science, many machine learning algorithms involve the optimization of weights, coefficients, and hyperparameters based on information contained in training data. We seek a principled method for evaluating the parmaeter space, such that consecutive samples are taken from regions of the search space that are more likely to contain minima.\n",
    "\n",
    "One such approach to global optimization is **Bayesian Optimization**. As the name implies, this approach uses Bayes Theorem to direct the parameter search, and is well-suited to *black box* objective functions that are complex, noisy, and/or expensive to evaluate, particularly where one does not have access to derivatives, or when the problem at hand is non-convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a probabilistic model for the objective.\n",
    "Include hierarchical structure about units, etc.\n",
    "‣ Compute the posterior predictive distribution.\n",
    "Integrate out all the possible true functions.\n",
    "We use Gaussian process regression\n",
    "\n",
    "Optimize a cheap proxy function instead.\n",
    "The model is much cheaper than that true objective.\n",
    "The main insight:\n",
    "Make the proxy function exploit uncertainty to balance\n",
    "exploration against exploitation.\n",
    "\n",
    "Gaussian process (GP) is a distribution on functions.\n",
    "‣ Allows tractable Bayesian modeling of functions\n",
    "without specifying a particular finite basis.\n",
    "‣ Input space (where we’re optimizing) \n",
    "\n",
    "## Using Uncertainty in Optimization\n",
    "\n",
    "$$x ^ { \\star } = \\arg \\min _ { x \\in \\mathcal { X } } f ( x )$$\n",
    "\n",
    "We can evaluate the objective pointwise, but do not\n",
    "have an easy functional form or gradients.\n",
    "‣ After performing some evaluations, the GP gives us\n",
    "easy closed-form marginal means and variances\n",
    "\n",
    "Exploration: Seek places with high variance.\n",
    "‣ Exploitation: Seek places with low mean.\n",
    "‣ The acquisition function balances these for our proxy\n",
    "optimization to determine the next evaluation\n",
    "\n",
    "The GP posterior gives a predictive mean function $\\mu(x)$\n",
    "and a predictive marginal variance function $\\sigma(x)$\n",
    "\n",
    "$$\\gamma ( x ) = \\frac { f \\left( x _ { \\text {best } } \\right) - \\mu ( x ) } { \\sigma ( x ) }$$\n",
    "\n",
    "Probability of improvement\n",
    "\n",
    "$$a _ { \\mathrm { PI } } ( x ) = \\Phi ( \\gamma ( x ) )$$\n",
    "\n",
    "Expected improvement\n",
    "\n",
    "$$a _ { E 1 } ( x ) = \\sigma ( x ) ( \\gamma ( x ) \\Phi ( \\gamma ( x ) ) + \\mathcal { N } ( \\gamma ( x ) ; 0,1 ) )$$\n",
    "\n",
    "GP upper confidence bound\n",
    "\n",
    "$$a _ { L C B } ( x ) = \\mu ( x ) - \\kappa \\sigma ( x )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Is Bayesian Optimization?\n",
    "\n",
    "Bayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function.\n",
    "\n",
    "It is an approach that is most useful for objective functions that are complex, noisy, and/or expensive to evaluate.\n",
    "\n",
    "\n",
    "\n",
    "We can devise specific samples (x1, x2, …, xn) and evaluate them using the objective function f(xi) that returns the cost or outcome for the sample xi. Samples and their outcome are collected sequentially and define our data D, e.g. D = {xi, f(xi), … xn, f(xn)} and is used to define the prior. The likelihood function is defined as the probability of observing the data given the function P(D | f). This likelihood function will change as more observations are collected.\n",
    "\n",
    "P(f|D) = P(D|f) * P(f)\n",
    "\n",
    "The posterior represents everything we know about the objective function. It is an approximation of the objective function and can be used to estimate the cost of different candidate samples that we may want to evaluate.\n",
    "\n",
    "In this way, the posterior probability is a surrogate objective function.\n",
    "\n",
    "The posterior captures the updated beliefs about the unknown objective function. One may also interpret this step of Bayesian optimization as estimating the objective function with a surrogate function (also called a response surface).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Surrogate Function: Bayesian approximation of the objective function that can be sampled efficiently.\n",
    "The surrogate function gives us an estimate of the objective function, which can be used to direct future sampling. Sampling involves careful use of the posterior in a function known as the “acquisition” function, e.g. for acquiring more samples. We want to use our belief about the objective function to sample the area of the search space that is most likely to pay off, therefore the acquisition will optimize the conditional probability of locations in the search to generate the next sample.\n",
    "\n",
    "Acquisition Function: Technique by which the posterior is used to select the next sample from the search space.\n",
    "Once additional samples and their evaluation via the objective function f() have been collected, they are added to data D and the posterior is then updated.\n",
    "\n",
    "This process is repeated until the extrema of the objective function is located, a good enough result is located, or resources are exhausted.\n",
    "\n",
    "The Bayesian Optimization algorithm can be summarized as follows:\n",
    "\n",
    "1. Select a Sample by Optimizing the Acquisition Function.\n",
    "2. Evaluate the Sample With the Objective Function.\n",
    "3. Update the Data and, in turn, the Surrogate Function.\n",
    "4. Go To 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/bayes_opt_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of demonstration, the objective function we are going to consider is the Forrester et al. (2008) function:\n",
    "\n",
    "$$f(x) = (6x - 2)^2 \\sin(12x - 4)$$ \n",
    "$$x \\in [0,1]$$\n",
    "\n",
    "This function has both a local minimum and a global minimum. The global minimum is at $x^*=0.75725$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torch gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = lambda x: (6*x - 2)**2 * np.sin(12*x - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 1)\n",
    "plt.plot(x_range, objective(x_range));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([0.0, 0.1, 0.4, 0.9])\n",
    "y = objective(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Interval(0.0, 0.001))\n",
    "model = ExactGPModel(X, y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, likelihood, x, y, lr=0.01, n_iter=500, plot_loss=True):\n",
    "\n",
    "    _pbar_data = {'loss': 0}\n",
    "    _desc = \"Current loss = {loss:.4f}\\tOptimizing\"\n",
    "    pbar = tqdm(range(n_iter), desc=_desc.format(**_pbar_data))\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "#         {'params': model.parameters()},  \n",
    "        {\"params\": model.covar_module.parameters()},\n",
    "    ], lr=lr)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    if plot_loss:\n",
    "        losses = np.empty(n_iter)\n",
    "    for i in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = -mll(output, y)\n",
    "        if plot_loss: losses[i] = loss.item()\n",
    "        loss.backward()\n",
    "        if not i % 5:\n",
    "            _pbar_data['loss'] = loss.detach().cpu().numpy()\n",
    "            pbar.set_description(_desc.format(**_pbar_data))\n",
    "        optimizer.step()\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.plot(losses)\n",
    "        \n",
    "    return model, likelihood, mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, mll = fit(model, likelihood, X, y, lr=0.1, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(X.numpy(), y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_targets.numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def PI(model, likelihood):\n",
    "    # calculate the best surrogate score found so far\n",
    "    best = model.train_targets.numpy().min()\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_x = torch.linspace(0, 1, 51)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    mu, std = observed_pred.mean, observed_pred.stddev\n",
    "    gamma = (mu.numpy() - best) / (std.numpy()+1E-9)\n",
    "    # calculate the probability of improvement\n",
    "    probs = 1-norm.cdf(gamma)\n",
    "    plt.plot(test_x.numpy(), probs)\n",
    "    return test_x[np.argmax(probs)].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a _ { E 1 } ( x ) = \\sigma ( x ) ( \\gamma ( x ) \\Phi ( \\gamma ( x ) ) + \\mathcal { N } ( \\gamma ( x ) ; 0,1 ) )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EI(model, likelihood):\n",
    "    # calculate the best surrogate score found so far\n",
    "    best = model.train_targets.numpy().min()\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_x = torch.linspace(0, 1, 51)\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "    mu, std = observed_pred.mean.numpy(), observed_pred.stddev.numpy()\n",
    "    gamma = (mu - best) / (std+1E-9)\n",
    "    vals = std * (gamma*(1-norm.cdf(gamma)) + norm.pdf(gamma))\n",
    "    plt.plot(test_x.numpy(), vals)\n",
    "    opt = test_x[np.argmax(vals)].item()\n",
    "    plt.scatter(opt, np.max(vals))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI(model, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([0.0, 0.1, 0.3, 0.4, 0.63, 0.9])\n",
    "y = objective(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Interval(0.0, 0.001))\n",
    "model = ExactGPModel(X, y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, mll = fit(model, likelihood, X, y, lr=0.1, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(X.numpy(), y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI(model, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([0.0, 0.1, 0.3, 0.4, 0.52, 0.63, 0.72, 0.9])\n",
    "y = objective(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=gpytorch.constraints.Interval(0.0, 0.001))\n",
    "model = ExactGPModel(X, y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, mll = fit(model, likelihood, X, y, lr=0.1, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(X.numpy(), y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI(model, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(x_new):\n",
    "    y = objective(x_new) # evaluate f at new point.\n",
    "    X = torch.cat([model.train_inputs[0], \n",
    "                   torch.tensor(np.atleast_2d(x_new), dtype=torch.float)])\n",
    "    y = torch.cat([model.train_targets, \n",
    "                   torch.tensor(np.atleast_1d(y), dtype=torch.float)])\n",
    "    model.set_train_data(X, y)\n",
    "    # optimize the GP hyperparameters using Adam with lr=0.001\n",
    "    optimizer = torch.optim.Adam(gpmodel.parameters(), lr=0.001)\n",
    "    gp.util.train(gpmodel, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, noise=0.1):\n",
    "    noise = np.random.normal(loc=0, scale=noise)\n",
    "    return (x**2 * np.sin(5 * np.pi * x)**6.0) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid-based sample of the domain [0,1]\n",
    "X = np.arange(0, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the domain without noise\n",
    "y = [objective(x, 0) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the domain with noise\n",
    "ynoise = [objective(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.argmax(y)\n",
    "print('Optima: x=%.3f, y=%.3f' % (X[ix], y[ix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization using pyGPGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyGPGO is one of several general-purpose Python libraries for conducting Bayesian optimization. This is a short demonstration of how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Franke's function** as an optimization target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    # Franke's function (https://www.mathworks.com/help/curvefit/franke.html)\n",
    "    one = 0.75 * np.exp(-(9 * x - 2)**2/4 - (9 * y - 2)**2/4)\n",
    "    two = 0.75 * np.exp(-(9 * x + 1)**2/49 - (9 * y + 1)/10)\n",
    "    three = 0.5 * np.exp(-(9 * x - 7)**2/4 - (9*y - 3)**2/4)\n",
    "    four = 0.25 * np.exp(-(9 * x - 4)**2 - (9*y - 7)**2)\n",
    "    return one + two + three - four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple visualization of the latter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "x = np.linspace(0, 1, num=1000)\n",
    "y = np.linspace(0, 1, num=1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                           linewidth=0)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The empirical Bayes approach (marginal log-likelihood)\n",
    "\n",
    "We use the Matérn $\\nu=3/2$ as our covariance function, a Gaussian Process surrogate and the expected improvement acquisition function. Notice we call the `GaussianProcess` class with arguments `optimize=True` and `usegrads=True`, which specifies that we want to optimize the marginal log-likelihood using exact gradients (`usegrads=False` would approximate them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyGPGO.covfunc import matern32\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.GPGO import GPGO\n",
    "\n",
    "cov = matern32()\n",
    "gp = GaussianProcess(cov, optimize=True, usegrads=True)\n",
    "acq = Acquisition(mode='ExpectedImprovement')\n",
    "param = {'x': ('cont', [0, 1]),\n",
    "         'y': ('cont', [0, 1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the opimization process as usual and get our results back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "gpgo = GPGO(gp, acq, f, param)\n",
    "gpgo.run(max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpgo.getResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fully-Bayesian approach\n",
    "\n",
    "Instead of optimizing the marginal log-likelihood, a fully Bayesian implementation takes into account the uncertainty in the parameters in the optimization procedure by assigning priors and estimating them. The process is identical, except that we change our `GaussianProcess` class by another that implements MCMC sampling (via `pyMC3`). \n",
    "\n",
    "We use slice sampling in this example for 300 iterations using a burnin of 100 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyGPGO.surrogates.GaussianProcessMCMC import GaussianProcessMCMC\n",
    "import pymc3 as pm\n",
    "\n",
    "gp = GaussianProcessMCMC(cov, niter=300, burnin=100, step=pm.Slice)\n",
    "acq = Acquisition(mode='IntegratedExpectedImprovement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure now is exactly the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gpgo = GPGO(gp, acq, f, param)\n",
    "gpgo.run(max_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the posterior distribution of the hyperparameters $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpgo.GP.posteriorPlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then query the estimated location of the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpgo.getResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "In this tutorial, we will learn the basics of the Bayesian optimization (BO) framework through a step-by-step example in the context of optimizing the hyperparameters of a binary classifier. But first of all, where is Bayesian optimization useful?\n",
    "\n",
    "There are a lot of case scenarios, one would typically use the BO framework in situations like:\n",
    "\n",
    "* The objective function has no closed-form\n",
    "* No gradient information is available\n",
    "* In presence of noise\n",
    "\n",
    "The BO framework uses a surrogate model to approximate the objective function and chooses to optimize it instead according to a chosen criteria. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating some synthetic data that we will use later for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "np.random.seed(20)\n",
    "X, y = make_moons(n_samples = 200, noise = 0.1) # Data and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let's visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cm_bright = ListedColormap(['#fc4349', '#6dbcdb'])\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, cmap = cm_bright)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we want to use a Support Vector Machine (SVM) with the radial basis function kernel classifier on this data, which has two usual parameters to optimize, $C$ and $\\gamma$. We need to first define a target function that takes these two hyperparameters as input and spits out an error (e.g, using some form of cross validation). Define also a dictionary, specifying parameters and input spaces for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def evaluateModel(C, gamma):\n",
    "#     clf = SVC(C=10**C, gamma=10**gamma)\n",
    "#     return np.average(cross_val_score(clf, X, y))\n",
    "\n",
    "# params = {'C':      ('cont', (-4, 5)),\n",
    "#           'gamma':  ('cont', (-4, 5))\n",
    "#          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def evaluate_model(hidden_size_1, hidden_size_2):\n",
    "\n",
    "    hidden_size_1 = int(hidden_size_1)\n",
    "    hidden_size_2 = int(hidden_size_2)\n",
    "    \n",
    "    # Build a feed-forward network\n",
    "    model = nn.Sequential(nn.Linear(2, hidden_size_1),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_size_1, hidden_size_2),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(hidden_size_2, 1),\n",
    "                          nn.Sigmoid())\n",
    "    \n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for t in range(1000):\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(64, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'hidden_size_1':      ('int', [4, 64]),\n",
    "          'hidden_size_2':  ('int', [4, 64])\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyGPGO.covfunc import squaredExponential\n",
    "sexp = squaredExponential()\n",
    "gp = GaussianProcess(sexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify now an _acquisition function_, that will determine the behaviour of the BO procedure when selecting a new point. For instance, it is very common to use the Expected Improvement (EI) acquisition, that will both take into account the probability of improvement of a point and its magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq = Acquisition(mode = 'ExpectedImprovement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! Finally call the `GPGO` class and put everything together. We'll run the procedure for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpgo = GPGO(gp, acq, evaluate_model, params)\n",
    "gpgo.run(max_iter = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally retrieve your result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpgo.getResult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Rasmussen, C. E., & Williams, C. K. I. (2004). Gaussian processes for machine learning. International journal of neural systems (Vol. 14). http://doi.org/10.1142/S0129065704001899\n",
    "\n",
    "Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian Optimization of Machine Learning Algorithms. Adv. Neural Inf. Process. Syst. 25, 1–9. http://doi.org/2012arXiv1206.2944S\n",
    "\n",
    "Jiménez J., Ginebra, J. (2017). Bayesian optimization in machine-learning. http://hdl.handle.net/2117/105999\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
